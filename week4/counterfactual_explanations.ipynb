{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📅 Week 4 -Counterfactual Explanations using Tabular Data\n",
        "#### 🚨 **First things first! Make a copy of this notebook. Your changes will not save unless you create your own copy!**🚨\n"
      ],
      "metadata": {
        "id": "ZsEtuxqZFGnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are a data scientist and risk modeler of a new fintech company that provides personal loans to individuals. As a startup, the company has limited staff. For this reason, your company decides to create a machine learning algorithm that can assess risk and categories potential loan takers in to low and high risk category. \n",
        "\n",
        "However, the company has a small pool of data from the manual decisions it made. Despite these challenges, your team is determined to develop a state-of-the-art loan default/risk prediction model to differentiate the company from its competitors.\n",
        "\n",
        "<center><img src = 'https://media1.giphy.com/media/3orif2cYsDAMzFMvjW/giphy.gif?cid=ecf05e47d9yxi8z0ftvw5ig3xytkvdky6oczi3uyrc4qet32&rid=giphy.gif&ct=g'></center>\n",
        "\n",
        "The stakes are high as there is a risk of losing out on potential customers and damaging the reputation of the company in case of false rejections. Also, you want to inform your customers why exactly were they rejected and suggest suitable actionables they can take to lower their risk and hypothetically get the credit loan. Your team decides that the obvious choice would be to generate counterfactuals using XAI principles and present them to customers. \n",
        "\n",
        "Therefore, your job as the data scientist is to build a machine learning model that predicts risk on the manual data. To generate and verify if the counterfactuals are reasonable and are plausible."
      ],
      "metadata": {
        "id": "dWXumWi8VgHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#📦 Installation and Imports\n",
        "We will use same [OmnixAI](https://github.com/salesforce/OmniXAI) python package for generating counterfactuals"
      ],
      "metadata": {
        "id": "nvvLfjevt72K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omnixai"
      ],
      "metadata": {
        "id": "SbqfNp19N5k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The Usual Suspects\n",
        "import tensorflow as tf\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Any\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "## For visualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', size=14)\n",
        "\n",
        "## Training pytorch tabular model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "## OmniXAI Counterfactual Explainer\n",
        "from omnixai.data.tabular import Tabular\n",
        "from omnixai.explainers.tabular import CounterfactualExplainer\n",
        "from omnixai.explainers.tabular.specific.decision_tree import TreeClassifier"
      ],
      "metadata": {
        "id": "7gqugzHcNJQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💻Dataset - German Credit Risk\n",
        "For developing a machine learning model, you decide to use a [real data](https://archive.ics.uci.edu/ml/datasets/South+German+Credit) and understand the factors that influenced the decision of credit risk. For this project, we will be using the German Credit Risk dataset. Download the dataset from [Kaggle here](https://www.kaggle.com/datasets/kabure/german-credit-data-with-risk) or just run the cell below. "
      ],
      "metadata": {
        "id": "ckMILS-kNJ7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Read CSV file and import to dataframe from the url\n",
        "url = 'https://drive.google.com/file/d/13vAvup3zgmkPOJ9P4ulRkQ3BQCN7nqe_/view?usp=sharing'\n",
        "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
        "df = pd.read_csv(path, index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HRdxCo0ZMGDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🔎 Exploratory Data Analysis\n",
        "Let us understand this dataset and make it ready for our ML model. The attributes are pretty much self-explanatory. However, there are some `NaN` in the data. Let us check how many columns have missing information by exploring this dataset"
      ],
      "metadata": {
        "id": "IPygv7EavYfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "k_92P7rV-v7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "OoV7mTpsvPq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only categorical variables (Dtype - `object`) have missing entries. For these are categorical variables, we will fill the NaN with an additional category called `other`"
      ],
      "metadata": {
        "id": "a9bx0rCJw1nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: For the columns with missing information, fill the NaN with the variable 'other'\n",
        "# df.fillna(..., inplace = True)\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "sSEIh5G_wAYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "V6FH2yQBan-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset now has 1000 rows that are valid with no NaNs. The column `Risk` is the predictor of interest that catergorizes the individual based on different attributes into `good` or `bad`. Let us see the number of people with `good` and `bad` risk profile"
      ],
      "metadata": {
        "id": "kxo31b8GxcRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Risk'].value_counts().plot(kind ='bar')"
      ],
      "metadata": {
        "id": "at7cpROdpUkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Categorical Variables"
      ],
      "metadata": {
        "id": "ARjbZqlNbWyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert categorical variables with two classes to binary format\n",
        "In binary format, we assign a class with label `0` and other class with `1`"
      ],
      "metadata": {
        "id": "R02hzJMgAGlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TO DO: Convert `Risk` and `Sex` to a binary variable\n",
        "## Use .map() method and convert bad -> 0 and good -> 1 & male ->1 and female ->0\n",
        "\n",
        "# df['Risk'] = df['Risk'].map(...)\n",
        "# df['Sex'] = df['Sex'].map(...)"
      ],
      "metadata": {
        "id": "h4haytcZ1ytC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Convert categorical variables with >2 classes to ordinal format\n",
        "In ordinal format, we assign a number for each class starting from 0. The columns are `Job` ,  `Housing` , `Saving accounts` , `Checking account`, and `Purpose`. Let us explore each of them"
      ],
      "metadata": {
        "id": "I8dvFHc9AKsW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_variables = ['Job' , 'Housing' , 'Saving accounts' , 'Checking account', 'Purpose']\n",
        "fig, ax = plt.subplots(nrows=2,ncols=3, figsize= (14,10))\n",
        "for i,category in enumerate(cat_variables):\n",
        "  j = i if i < 3 else i % 3\n",
        "  df[category].value_counts().plot(kind = 'bar', ax = ax[int(i/3),j], title = category)\n",
        "fig.delaxes(ax[1,2])\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "UHL9BcimCING"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the `Job` column is already represented in an ordinal format. Let us convert the rest of them too. In order to keep track of the relationship of classe and labels, let us store the class dictionaries in `class_to_labels` variable all categories"
      ],
      "metadata": {
        "id": "ivGhNviccH_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_labels = {}\n",
        "cat_to_ordinal = ['Housing' , 'Saving accounts' , 'Checking account', 'Purpose']\n",
        "for category in cat_to_ordinal:\n",
        "    values = df[category].unique()\n",
        "    ids = range(0,len(values))\n",
        "    cat_dict = dict(zip(values,ids))\n",
        "    df[category] = df[category].map(cat_dict)\n",
        "    class_to_labels[category] = cat_dict"
      ],
      "metadata": {
        "id": "EbsvcXIRZbg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "cosS-dzXbwXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_labels"
      ],
      "metadata": {
        "id": "HTE6Mb3mbLOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Distributions"
      ],
      "metadata": {
        "id": "a0JBtc77bqtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(\n",
        "    df, x=\"Age\", col=\"Risk\", row=\"Sex\",\n",
        "    binwidth=3, height=5, facet_kws=dict(margin_titles=True)\n",
        ")"
      ],
      "metadata": {
        "id": "vV_RcFVKYKKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize= (12,10))\n",
        "sns.violinplot(data=df, x=\"Risk\", y=\"Credit amount\", hue=\"Housing\", inner=\"box\", linewidth=1,pallette  = 'tab10')\n",
        "sns.despine(left=True)"
      ],
      "metadata": {
        "id": "ch2sRdCeaCeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Correlations"
      ],
      "metadata": {
        "id": "kXcZH3lDb0_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us see which attributes are correlated (positive/negative) with the `Risk`. To do this we need to convert the `Risk` to binary variable"
      ],
      "metadata": {
        "id": "jOP3h56e12m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TO DO: Use `seaborn` heatmap to display the correlations. Fill in the `visualize_corr` function\n",
        "## Make sure to display the annotations and colormap\n",
        "def visualize_corr(df, figsize = (12,12)):\n",
        "  plt.figure(figsize=figsize)\n",
        "  pass\n",
        "\n",
        "\n",
        "def visualize_corr(df, figsize = (12,12)):\n",
        "  plt.figure(figsize=figsize)\n",
        "  sns.heatmap(df.corr(), annot=True)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rsX--rz68cIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_corr(df)"
      ],
      "metadata": {
        "id": "ForTmwH44px-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚨TODO: Let's build some Intuition 🤔\n",
        "\n",
        "<img src ='https://cdn.streamelements.com/uploads/71a1c318-9fd1-4cf1-b4ea-d090a49cb85c.gif'>\n",
        "\n",
        "Based on the exploratory data analysis above, list the attributes/features that are most influential the deciding the `Risk` as good/bad.\n",
        "\n",
        "\n",
        "1.   \"List item here\"\n",
        "2.   \"List item here\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2oL7k0xx8USR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer the following questions\n",
        "1. List the attributes that make it less risky(good)\n",
        "2. List the attributes that make it more risky(bad)\n",
        "3. Did you notice any trends? Do they sound reasonable?\n"
      ],
      "metadata": {
        "id": "mSSVevF46lbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#🤖 Machine Learning Model\n",
        "Using the manual data as input, let us build our machine learning model. We will build a Neural Network based classifiers in this section. Before we proceed, we need to normalize our numnerical variables and split the dataset into `train` and `test` with a 80:20 split "
      ],
      "metadata": {
        "id": "hmFMSVz15YNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "24ubZ-7JAsZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Scale numerical variables to [0,1] using `MinMaxScalar()` from sklearn\n",
        "\n",
        "numeric_columns= ['Age','Credit amount','Duration']\n",
        "# Scaler = ...\n",
        "# df[numeric_columns] = ...\n"
      ],
      "metadata": {
        "id": "CZ9qMXLf_Wkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Note: You can convert to the original value using the `inverse_transform`\n",
        "Scaler.inverse_transform(df.head()[numeric_columns])"
      ],
      "metadata": {
        "id": "Fq5ehiS3A-KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Split the data into train and test. Initialize variables X & Y and pass into `train_test_split` function\n",
        "## Make sure to fill the remaining blanks and use a random_state\n",
        "\n",
        "# X = ...\n",
        "# Y = ...\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=..., random_state=..., stratify=y)\n"
      ],
      "metadata": {
        "id": "ywqe8naAyzIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Here is the function to build and train the nueral network model\n",
        "def train_tf_model(x_train, y_train, x_test, y_test):\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 2)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 2)\n",
        "\n",
        "    model = tf.keras.models.Sequential()\n",
        "    ### Fill out the inpout size based on the number of input variables\n",
        "    model.add(tf.keras.layers.Input(shape=(9,)))\n",
        "    model.add(tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu))\n",
        "    model.add(tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu))\n",
        "    model.add(tf.keras.layers.Dense(units=2, activation=tf.keras.activations.softmax))\n",
        "\n",
        "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=0.1,\n",
        "        decay_steps=1,\n",
        "        decay_rate=0.99,\n",
        "        staircase=True\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.SGD(\n",
        "        learning_rate=learning_rate,\n",
        "        momentum=0.9,\n",
        "        nesterov=True\n",
        "    )\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=0)\n",
        "    train_loss, train_accuracy = model.evaluate(x_train, y_train, batch_size=51, verbose=0)\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test, batch_size=51, verbose=0)\n",
        "\n",
        "    print('Train loss: {:.4f}, train accuracy: {:.4f}'.format(train_loss, train_accuracy))\n",
        "    print('Test loss:  {:.4f}, test accuracy:  {:.4f}'.format(test_loss, test_accuracy))\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZDXzxtOQypIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build and train the nueral network\n",
        "model_nn = train_tf_model(x_train, y_train, x_test, y_test)"
      ],
      "metadata": {
        "id": "KdJo_ghb3CS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn.predict(x_test[:5])"
      ],
      "metadata": {
        "id": "lLteCoRP0U7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔄️ Generate Counterfactuals\n",
        "Now that we have our ML model ready, let us generate our CounterfactualExplainer using OmnixAI library. Read this [paper](https://arxiv.org/ftp/arxiv/papers/1711/1711.00399.pdf) for more information on counterfactuals"
      ],
      "metadata": {
        "id": "TPmeiveekDsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## OmnixAI requires the data to be in a Tabular, Image or Text format. Let's convert our Dataframe to a Tabular format\n",
        "## TODO: Pass the training data and features\n",
        "\n",
        "# feature_names = ...\n",
        "\n",
        "## Used for initializing the explainer\n",
        "# tabular_data_train = Tabular(\n",
        "#     data = ...,\n",
        "#     feature_columns=feature_names\n",
        "# )\n",
        "\n",
        "# tabular_data_test = Tabular(\n",
        "#     data = ...,\n",
        "#     feature_columns=feature_names\n",
        "# )"
      ],
      "metadata": {
        "id": "rwWi19shygwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Pass the training data and model to the Counterfactual explainer\n",
        "# explainer_nn = CounterfactualExplainer(training_data=...,\n",
        "#                                        predict_function=...)\n"
      ],
      "metadata": {
        "id": "W4B2rpPO3RXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Time to generate some counterfactuals!!!\n",
        "explanations = explainer_nn.explain(tabular_data_test[1])"
      ],
      "metadata": {
        "id": "hiPu0nkj4qdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations.ipython_plot(index = 0)"
      ],
      "metadata": {
        "id": "47GV6LZ3OcRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Sample few records in the test data where the `Risk` is predicted as `bad`(0) and generate a counterfactuals\n",
        "## NOTE: Select a sample size N if the explainer takes long\n",
        "\n",
        "# N = ...\n",
        "# RANDOM_STATE = ...\n",
        "# tab_indices = y_test[y_test == ...]\\\n",
        "# .sample(n=..., random_state=...)\\\n",
        "# .index\\\n",
        "# .tolist()\n",
        "\n",
        "N = 1\n",
        "RANDOM_STATE = 42\n",
        "tab_indices = y_test[y_test == 0].sample(n=N, random_state=RANDOM_STATE).index.tolist()"
      ],
      "metadata": {
        "id": "yGRdIezJonGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations_risk_0 = explainer_nn.explain(tabular_data_test.to_pd().loc[tab_indices])"
      ],
      "metadata": {
        "id": "RtHU7pCxPZbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations_risk_0.ipython_plot()"
      ],
      "metadata": {
        "id": "smvj_IG0PwgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Find sample records in the train & test data where the model made an incorrect/different prediction and generate counterfactuals for those predictions\n",
        "## Fill out the missing entries in the function\n",
        "\n",
        "# def find_incorrect_pred_indices(\n",
        "#     model: Any, X: pd.DataFrame, y: pd.Series, size: int = N\n",
        "# ) -> None:\n",
        "#     \"\"\"\n",
        "#     Find the incorrect predictions from a model and generates the counterfactual\n",
        "#     explanations for the particular dataset that the model was trained/evaluated on.\n",
        "    \n",
        "#     Args:\n",
        "#         model: The model used for training\n",
        "#         X:     The dataset used on the model training/evaluation excluding the target column\n",
        "#         y:     The target column values of X\n",
        "#         size:  The size of the random samples to select from each of the false positives and false negatives.\n",
        "#                The bigger the size, the longer the computation of the counterfactuals. \n",
        "#     \"\"\"\n",
        "#     predictions = model.predict(X).argmax(axis=-1)\n",
        "#     actual = y.values\n",
        "#     difference = actual - predictions\n",
        "#     false_positives = np.where(difference == ...)[0]\n",
        "#     false_negatives = np.where(difference == ...)[0]\n",
        "#     assert size < len(false_positives)\n",
        "#     assert size < len(false_negatives)\n",
        "#     random_sample_false_positives = np.random.choice(..., size=...)\n",
        "#     random_sample_false_negatives = np.random.choice(..., size=...)\n",
        "#     flattened_indices = list(\n",
        "#         itertools.chain.from_iterable((random_sample_false_positives, random_sample_false_negatives))\n",
        "#     )\n",
        "#     tab_indices = X.iloc[flattened_indices].index.tolist()\n",
        "#     return tab_indices"
      ],
      "metadata": {
        "id": "OtUEhXNS3ToY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tab_indices_train = find_incorrect_pred_indices(model=model_nn, X=x_train, y=y_train,size =1)\n",
        "tab_indices_test = find_incorrect_pred_indices(model=model_nn, X=x_test, y=y_test,size = 1)"
      ],
      "metadata": {
        "id": "yldGHAEKTESU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations_train = explainer_nn.explain(tabular_data_train.to_pd().loc[tab_indices_train])\n",
        "explanations_test = explainer_nn.explain(tabular_data_test.to_pd().loc[tab_indices_test])"
      ],
      "metadata": {
        "id": "3l2UeZiNTb7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations_train.ipython_plot()"
      ],
      "metadata": {
        "id": "bzvbFEtVTnv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanations_test.ipython_plot()"
      ],
      "metadata": {
        "id": "icXS4jHWTvt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🗝️Outro\n",
        "Awesome! You made it till the end 👏 Answer the following questions to deepen your understanding.\n",
        "\n",
        "1. Do you think the ML model prediction is similar to the orignal manual prediction? Do the predictions align well with your initial intuition?\n",
        "2. We as humans have biases in our decision making. The biases might seep into the ML model as the models try to minimize the loss and improve accuracy. With the help of counterfactuals, did you see that the model has any inherent bias?\n",
        "3. Do you think the counterfactuals serve a good tool to explain customers what they could do to achieve a better Risk profile? Give reasoning."
      ],
      "metadata": {
        "id": "jwf9Dt2X4S_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💰Bonus - Counterfactuals for text classification \n",
        "Let's apply counterfactuals in the context of text classification. Specifically, we will generate counter factuals for the sentiment analysis we used in Week 2 for movie reviews. We will use the same pretrained `cardiffnlp/twitter-xlm-roberta-base-sentiment` model from hugging face. The OmnixAI provides [polyjuice](https://github.com/tongshuangwu/polyjuice) explainer for counterfactuals. You can find a demonstration of `polyjuice` from Hugging Face [here](https://media1.giphy.com/media/v7yls1pusVAyo2JfPu/giphy.gif?cid=ecf05e47yq0khgmvfuggu2tukla3eavittk2oyxagnd51llz&rid=giphy.gif&ct=g)\n",
        "\n",
        "\n",
        "<center><img src='https://media1.giphy.com/media/v7yls1pusVAyo2JfPu/giphy.gif?cid=ecf05e47yq0khgmvfuggu2tukla3eavittk2oyxagnd51llz&rid=giphy.gif&ct=g'></center>"
      ],
      "metadata": {
        "id": "wYudf5tMkGxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.6.0 polyjuice_nlp torch omnixai"
      ],
      "metadata": {
        "id": "LIMa1n4ggVXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTk is a NLP toolkit that provides helpful lexical resources such as the wordnet (https://wordnet.princeton.edu/) which can be used to find synsets of words. Eg. car <--> automobile\n",
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "pSLhj9X7dVvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from polyjuice import Polyjuice\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from omnixai.data.text import Text\n",
        "from omnixai.explainers.nlp import NLPExplainer"
      ],
      "metadata": {
        "id": "FGjSoWAhBsxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Before we build our transformer, lets make sure to setup the device.\n",
        "## To run this notbeook via GPU: Edit -> Notebook settings -> Hardware accelerator -> GPU\n",
        "## If your GPU is working, device is \"cuda\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "p02WE4Yqhc7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\" \n",
        "# The pre- and post-processing functions\n",
        "preprocess = lambda x: x.values\n",
        "postprocess = lambda outputs: np.array([[s[\"score\"] for s in ss] for ss in outputs])\n",
        "\n",
        "\n",
        "##TODO: Build pre-trained model for sentiment analysis\n",
        "\n",
        "# model = transformers.pipeline(\n",
        "#     'sentiment-analysis',\n",
        "#     model=...,\n",
        "#     return_all_scores=...\n",
        "# )"
      ],
      "metadata": {
        "id": "poK9H_P8hXw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build explainer using the NLPExplainer. Use \"polyjuice\" for explainer\n",
        "\n",
        "# explainer = NLPExplainer(\n",
        "#     explainers=[...],\n",
        "#     mode=\"...\",\n",
        "#     model=...,\n",
        "#     preprocess=...,\n",
        "#     postprocess=...\n",
        "# )\n"
      ],
      "metadata": {
        "id": "oUkDIRTH8Bqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Remember our classes for sentiment analysis are as follows\n",
        "model.model.config.label2id"
      ],
      "metadata": {
        "id": "nHMzJcjv5WBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us use some phrases from movie reviews. Feel free to add your own or  ask ChatGPT to generate some for you😉"
      ],
      "metadata": {
        "id": "e8mr841N9gUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = Text([\n",
        "    \"What a great movie!\",\n",
        "    \"The movie had great narration and visuals despite a boring storyline\"\n",
        "])"
      ],
      "metadata": {
        "id": "oyM6_zZ4kcsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates explanations\n",
        "local_explanations = explainer.explain(x)"
      ],
      "metadata": {
        "id": "yYvOhcohjASj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## View explanations\n",
        "local_explanations['polyjuice'].ipython_plot(index = 1)"
      ],
      "metadata": {
        "id": "P0Q7UmWKqH0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the final project of the four week `Interpreting Machine Learning Models` course. We hope you had a fun learning experience. Keep engaged with the community and build it stronger💪"
      ],
      "metadata": {
        "id": "dfo85YO54-1x"
      }
    }
  ]
}