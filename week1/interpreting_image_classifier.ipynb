{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BWQ2MzN6WYe"
      },
      "source": [
        "# Week 1 - Interpreting Image Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the week 1 project of the Interpreting Machine Learning Models course! We are excited to help you unravel the mysteries behind machine learning algorithms."
      ],
      "metadata": {
        "id": "9R2KW2e0gIKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction - Week 1 Challenge"
      ],
      "metadata": {
        "id": "QyVWpAmGlF1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's 2050 and a mysterious virus has caused the global cat population to become hilariously clumsy and forgetful. People can often be seen watching in amusement as their feline companions stumble into walls, accidentally headbutt their own tails, and knock over everything in their way!\n",
        "\n",
        "<center><img src='https://media.tenor.com/FJgMcZ8QcvMAAAAM/epic-fail-fall.gif'></center>\n",
        "\n",
        "The situation quickly becomes frustrating for cat owners. Cats can no longer be left alone, as they are prone to forgetting where they put their toys and treats, and are even known to accidentally lock themselves in closets and bathrooms.\n",
        "\n",
        "To address this problem, cat owners decide to deploy cameras equipped with machine vision to detect and track the activities of these forgetful felines. However, they want to make sure the algorithm can accurately identify cats and doesn't raise false alarms, especially while the owners are napping. To achieve this, they hire a machine learning expert(you, yes you) to interpret the algorithm."
      ],
      "metadata": {
        "id": "Rs_gFaLAizxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We need you! [TODO]"
      ],
      "metadata": {
        "id": "e0VxLjMIlJwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are given a pre-trained ResNet model that is trained on Imagenet 1k dataset. Your task is to interpret \"Why the ResNet model detects cats?\"\n",
        "\n",
        "For interpreting a classification task, there are multiple dimensions to choose from (Global vs Local, Model agnostic vs. specific, Inherent vs. post hoc). We will be using a Model agnostic post hoc method and deploy it at a local scale\n",
        "\n",
        "Specifically, we will use LIME, SHAP, and integrated-gradient in this project. For each of these algorithms, you will be documenting the compute time and visualizing their explanations. At the end of the project, you'll be comparing the three evaluation approaches and assessing which you agree with most. So let's dive in!"
      ],
      "metadata": {
        "id": "OWqyx_kHi_IJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMc1B4o1urim"
      },
      "source": [
        "## Setup\n",
        "Before we start our mission, lets gets some gear set up. Firstly, lets install the missing packages and import the necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1dFx3pWqiXs"
      },
      "source": [
        "### Installation of Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7caVWltC6hUv"
      },
      "outputs": [],
      "source": [
        "!pip install omnixai\n",
        "!pip install dash\n",
        "!pip install dash-bootstrap-components\n",
        "## For local tunnel to a proxy server \n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOmqoiNrqqv7"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will import some usual suspects. We will use Pillow Image library to laod/create images. Finally, let us import our main weapon. Let us use [OmniXAI](https://opensource.salesforce.com/OmniXAI/latest/index.html) (Omni eXplainable AI), a Python library for explainable AI (XAI)."
      ],
      "metadata": {
        "id": "PhOmsFd1ddZ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_ZCAeu_6dYH"
      },
      "outputs": [],
      "source": [
        "## The usual suspects\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "import pickle\n",
        "\n",
        "## To build our classifer\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "\n",
        "## Pillow Library Image function alias PilImage\n",
        "from PIL import Image as PilImage\n",
        "\n",
        "## Omnixai library to build our explainer\n",
        "from omnixai.preprocessing.image import Resize\n",
        "from omnixai.data.image import Image\n",
        "from omnixai.explainers.vision import VisionExplainer\n",
        "from omnixai.visualization.dashboard import Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIFbVqpukVN"
      },
      "source": [
        "## Image Data and Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1tgSQQE6nsH"
      },
      "outputs": [],
      "source": [
        "## Let's start by loading the image that we want to explain\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "download = requests.get(url, stream=True).raw\n",
        "\n",
        "## TODO: Read the image using Pillow and convert the image into RBG\n",
        "### Hint: Use PilImage to read and convert\n",
        "\n",
        "# image = Image(...)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Print the image shape and view the image\n",
        "\n",
        "## Print the image shape\n",
        "# print(...)\n",
        "\n",
        "# Now, let's view it\n",
        "image.to_pil()\n",
        "# Shh! They are napping..."
      ],
      "metadata": {
        "id": "ddqyZv2mjFYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEq2_rZY6qj5"
      },
      "outputs": [],
      "source": [
        "## Before we build our classifier, lets make sure to setup the device.\n",
        "## To run this notbeook via GPU: Edit->Notebook settings ->Hardware accelerator -> GPU\n",
        "## If your GPU is working, device is \"cuda\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Lets build our classification model. We will use pre-trained ResNet34 model from PyTorch torchvision models.\n",
        "## Make sure to load the model onto the device for gpu\n",
        "\n",
        "# model = ..."
      ],
      "metadata": {
        "id": "K67JdQNgijpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets get a summary of our model using torchsummary\n",
        "from torchsummary import summary\n",
        "## TODO: Print the model summary\n",
        "### Hint: Use image shape for input_size\n",
        "# summary(...)"
      ],
      "metadata": {
        "id": "mk2KQ6e5uQHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Did you notice the last layer had 1000 classes. Lets import all the classes. \n",
        "## We will later pass this to our explainer\n",
        "classes_url = 'https://gist.githubusercontent.com/DaniFojo/dad37f5bf00ddeb56ed36daf561dbf69/raw/bd006b86300a5886ac7f897a44b0525b75a4b5a1/imagenet_labels.json'\n",
        "imagenet_classes = json.loads(requests.get(classes_url).text)\n",
        "idx2label =  {int(k):v for k,v in imagenet_classes.items()}\n",
        "\n",
        "first_label = idx2label[next(iter(idx2label))]\n",
        "print(f\"The first class label from the ImageNet dataset is: '{first_label}'\")"
      ],
      "metadata": {
        "id": "hLTbu2GHLdT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmyv-bo3u9c8"
      },
      "source": [
        "## Buiding our Explainer\n",
        "\n",
        "To build our Explainer for our model, we will use [Vision Explainer](https://opensource.salesforce.com/OmniXAI/v1.2.3/omnixai.explainers.vision.html) by OmniXAI. The explainer needs some pre-processing and post-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnrmP6R1txc0"
      },
      "source": [
        "### Pre-processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGpN6fIttv1O"
      },
      "outputs": [],
      "source": [
        "## TODO: Build the pre-processor pipeline for the explainer\n",
        "\n",
        "# The preprocessing function should convert the image to a Tensor \n",
        "# and then Normalise it\n",
        "\n",
        "# 1. Compose the transformations\n",
        "# transform = transforms.Compose([\n",
        "    ## 1a. write code to convert the image to tensor\n",
        "    #\n",
        "    ## 1b. write code to normalize the image\n",
        "    # \n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Create the preprocess logic using the transformation built in previous cell\n",
        "### Hint: Use torch.stack and load the images to the device\n",
        "\n",
        "# def preprocess(images):\n",
        "#   \"\"\"\n",
        "#   Args:\n",
        "#     images: Sequence of images to preprocess using the composed \n",
        "#             transformations created above\n",
        "\n",
        "#   Returns:  \n",
        "#     preprocessed_images: Sequence of preprocessed images\n",
        "#   \"\"\"\n",
        "#   preprocessed_images = ...\n",
        "#   return preprocessed_images"
      ],
      "metadata": {
        "id": "zoVmrOM1vdEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVriqWcuVSy"
      },
      "source": [
        "### Post-processor\n",
        "\n",
        "Next, we need to define our post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea4FZctmvow4"
      },
      "outputs": [],
      "source": [
        "## TODO: Build the post-processor function for the explainer\n",
        "# We will apply a softmax function to the logits obtained in the last layer\n",
        "# in order to convert the prediction scores to probabilities\n",
        "\n",
        "# def postprocess(logits):\n",
        "#   \"\"\"\n",
        "#   Args:\n",
        "#     logits: Logits from the last layer of the model\n",
        "  \n",
        "#   Returns:\n",
        "#     postprocessed_outputs: Output from the Softmax layer applied to the logits\n",
        "#   \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Z03RZ2vzVv"
      },
      "source": [
        "### Vision Explainer\n",
        "Now, construct the explainer using the VisionExplainer class. You'll want to provide it a list of the three explainer types you'd like to try: LIME, SHAP, and integrated gradient. Be sure to check the documentation for the appropriate arguments! See the sample code for VisionExplainer [here](https://opensource.salesforce.com/OmniXAI/v1.2.3/tutorials/vision.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R8gBAgJ6xK8"
      },
      "outputs": [],
      "source": [
        "#TODO: Build the VisionExplainer by filling in the blanks\n",
        "# explainer = VisionExplainer(\n",
        "#     explainers=[ ...],\n",
        "#     mode=\"...\",\n",
        "#     model=...,\n",
        "#     preprocess=...,\n",
        "#     postprocess=...,\n",
        "\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuRVgBBCwvZX"
      },
      "source": [
        "Now, we can generate some explanations for each of the explainers using the explainer.explain() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFR55IOvw5aT"
      },
      "outputs": [],
      "source": [
        "## Time to generate the explanations\n",
        "local_explanations = explainer.explain(Image(\n",
        "    data=np.concatenate([\n",
        "        image.to_numpy()]),\n",
        "    batched=True\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets write the local_explantions to a pickle file. We will use this in our dashboard\n",
        "with open('file.pkl', 'wb') as file:\n",
        "    # A new file will be created\n",
        "    pickle.dump(local_explanations, file)"
      ],
      "metadata": {
        "id": "33JVHA0COHCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL66926z7TA1"
      },
      "source": [
        "## Dashboard\n",
        "Now let's create a Dashboard to visualize our different explainers that we just built"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Google Colab hosts the server on remote local. Therefore, localhost on your machine will not lead you to the dashboard\n",
        "\n",
        "## Open `output.log` from files and use the link to get redirected. \n",
        "## <NOTE> : It might take a minute for the log file to show up. Hit refresh if need be.\n",
        "!nohup npx localtunnel --port 8000 > output.log &"
      ],
      "metadata": {
        "id": "MJhl4wR77IxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKdAir1y7hXD"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "###### Use the link from previous cell once running ######\n",
        "##########################################################\n",
        "\n",
        "\n",
        "## TODO: Fill in the Dashboard parameters\n",
        "\n",
        "# dashboard = Dashboard(\n",
        "#     instances=...,\n",
        "#     local_explanations=...,\n",
        "#     class_names= ...\n",
        "# )\n",
        "\n",
        "\n",
        "## Do not change the port number\n",
        "## <NOTE> Once you open the link, it might take a minute or two for the website to load fully. Be patient :)\n",
        "dashboard.show(port=8000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC_0U5yY7jVk"
      },
      "source": [
        "## Outro\n",
        "\n",
        "🎉Yay, you did it! Now that we've seen the explantions, you are ready to answer some questions about the various explanations!\n",
        "\n",
        "1. What are your thoughts on Interpretable AI?\n",
        "2. Compare the various explanations. Which method do you agree with most, why?\n",
        "3. Do you think the ResNet model is good enough for cat owners?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus (Extension)\n",
        "Document the computation time for each explainer: LIME, SHAP, and integrated-gradient."
      ],
      "metadata": {
        "id": "IALK072KVawY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets use hugging face cats vs dogs dataset\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "T2zBHxZv__wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now we will load 5 cat images from the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "## Feel free to change this number. In order to not run out of RAM we use 5 images\n",
        "NUM_IMAGES = 5\n",
        "dataset = load_dataset(\"cats_vs_dogs\")\n",
        "cats_data = dataset['train'][0:NUM_IMAGES]['image']\n",
        "cats_data"
      ],
      "metadata": {
        "id": "ZQXzI_tpjVeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Notice that the image sizes are different. \n",
        "## TODO: Convert them to same size using transforms.Resize\n",
        "\n",
        "#transform_resize = transforms.Compose([\n",
        "#    transforms.Resize(...)\n",
        "#])"
      ],
      "metadata": {
        "id": "ooQTgt7lRa_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Lets use the transformer and stack the images\n",
        "# TODO: Use `transform_resize` and `np.stack`\n",
        "\n",
        "# cats = ([... for cat in cats_data])"
      ],
      "metadata": {
        "id": "p8aaCZLuRufe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We will use this explainer function to create independant explainer \n",
        "def explainer(explainer):\n",
        "  return VisionExplainer(\n",
        "    explainers=[explainer],\n",
        "    mode=\"classification\",\n",
        "    model=model,\n",
        "    preprocess=preprocess,\n",
        "    postprocess=postprocess,\n",
        "  )"
      ],
      "metadata": {
        "id": "WpcEMXkq_yGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TODO: Initialize the explainer for 'Lime', 'SHAP', and 'integrated gradient'\n",
        "# lime = explainer(...)\n",
        "# shap = explainer(...)\n",
        "# ig = explainer(...)"
      ],
      "metadata": {
        "id": "xOxjBo3Zlpj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let us time the results. We will use built-in magic commands in jupyter \n",
        "%time lime_results = lime.explain(cats)"
      ],
      "metadata": {
        "id": "I2NN4qhXY8oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time shap_results = shap.explain(cats)"
      ],
      "metadata": {
        "id": "GbXgRgrimWED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time ig_results = ig.explain(cats)"
      ],
      "metadata": {
        "id": "RXj-xYLQmZ1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Google Colab hosts the server on remote local. Therefore, localhost on your machine will not lead you to the dashboard\n",
        "\n",
        "## Open `output.log` from files and use the link to get redirected. \n",
        "## <NOTE> : It might take a minute for the log file to show up. Hit refresh if need be.\n",
        "!nohup npx localtunnel --port 8000 > output.log &"
      ],
      "metadata": {
        "id": "ml_kvswYkOgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Combine all results\n",
        "combine_results = lime_results\n",
        "combine_results['shap'] = shap_results['shap']\n",
        "combine_results['ig'] = ig_results['ig']\n",
        "\n",
        "## Lets visualize the results on the Dashboard\n",
        "dashboard = Dashboard(\n",
        "    instances=Image(cats,batched =True),\n",
        "    local_explanations=combine_results,\n",
        "    class_names=idx2label\n",
        ")\n",
        "## Do not change the port\n",
        "## <NOTE> Once you open the link, it might take a minute or two for the website to load fully. Be patient :)\n",
        "dashboard.show(port=8000)"
      ],
      "metadata": {
        "id": "AsEFIWoCK3KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Thoughts🎉\n",
        "\n",
        "Congratulations on finishing the bonus sections. It is an impressive feat!\n",
        "\n",
        "---\n",
        "Please share your observations about the computation time for each of the explainers and recommend a method based on this and any other relevant factors, such as effectiveness or accuracy? If your recommendation differs from a previous suggestion, please explain the reason for this change."
      ],
      "metadata": {
        "id": "yz1qeEXWH2Yk"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}